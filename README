# Run the below command to install the ollama package to run open-source LLMs locally in our system.
# The current setup is tested on Ubuntu, so if using Windows, please use WSL.

curl -fsSL https://ollama.com/install.sh | sh

# Install the codellama:7b model.
# Execute the command inside the git repository to access the model file.

ollama create codellama:7b -f ./modelfile

# Once ollama is downloaded, run the startup script to set up the server environment.

./startup-script.sh

# Go to the localhost port 8000 to send requests to the server and beautify the readme file.
